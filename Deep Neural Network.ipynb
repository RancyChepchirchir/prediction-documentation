{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction: Deep Neural Network\n",
    "\n",
    "In this notebook we will develop a deep neural network to apply to the building energy prediction task. This is a supervised regression problem where the objective is build a model that is trained on past electricity consumption and weather information and then makes predictions for future energy consumption. \n",
    "\n",
    "Deep neural networks have gained immense popularity in recent years with extraordinary performance on many tasks. This includes above human level performance in computer vision - using convolutional neural networks - and natural language processing - using recurrent neural networks - problems. However, neural networks require a considerable amount of data points in order to learn the mapping from the features to the target, particularly as the depth of the network increases. For that reason, neural networks are typically not as succesful on small to medium sized datasets such as the building energy data used by EDIFES. The majority of the building datasets are under 1e6 observations, which may not be enough for a neural network to learn. Nonetheless, we will build a neural network that can then be tested on all of the buildings for performance relative to the other models.\n",
    "\n",
    "As in previous notebooks, we will go through the implementation step-by-step, and then refactor the code into a single function. The end objective is a function that can take in the training features, training targets, testing features, and testing targets, and return the model performance. This can then be integrate into the previous develop `evaluate_models` function. After the addition of the deep neural network to the set of models, there will be eight models that can be run on hundreds of building datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports \n",
    "\n",
    "We will use a standard stack of data science libraries: `pandas`, `numpy`, `sklearn`, `matplotlib` along with `keras` for the deep neural network implementation. See the `requirements.txt` file for the correct version of these libraries to install. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# numpy and pandas for data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Sklearn preprocessing functionality\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "\n",
    "# Matplotlib for visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Adjust default font size \n",
    "plt.rcParams['font.size'] = 18\n",
    "\n",
    "# Keras for neural networks\n",
    "from keras import models, layers, optimizers, losses, metrics, callbacks\n",
    "\n",
    "# Timer for recording runtime\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "# Utilities developed for project\n",
    "from utilities import preprocess_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/f-APS_weather.csv')\n",
    "train, train_targets, test, test_targets = preprocess_data(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Network\n",
    "\n",
    "A fully-connected deep neural network is made up of many layers consisting of matrix multiplies and bias additions. The outputs of the matrix multiplies are passed through a non-linearity which is where the neural network gets its power to approximate any function (provided it has enough hidden units). Each layer is made up of a number of units (also called neurons), generally a multiple of 2. The number of neurons per layer and the number of layers determine the model capacity: a model with more layers and more neurons per layer will have greater capacity to learn. However, the more total neurons, the greater the chance the model will overfit to the training data, especially with a limited amount of training data. A general technique is to start with a shallow network (a few layers) and keep adding more layers until the model overfits. Then, either remove layers as needed or employ some form of model regularization. Choosing an architecture is the most critical part of the deep neural network modeling, and the ideal architecture depends on the problem. We are going to use the same neural network across all buildings even though the best model probably differs significantly from building to building. Optimizing neural network architectures automatically is an open research question, and applying a random search through model architectures might be one option although for this problem we will only use one architecture.\n",
    "\n",
    "# Deep Neural Network Hyperparameters\n",
    "\n",
    "There are many hyperparameters we need to choose for a deep neural network:\n",
    "\n",
    "* Number of layers\n",
    "* Number of units per layer\n",
    "* Non-linear activation on hidden layers\n",
    "* Regularization methods\n",
    "* Optimizer (algorithm) used for minimizing the objective function\n",
    "* Learning rate of the optimizer\n",
    "* Number of training iterations\n",
    "* Batch size\n",
    "\n",
    "## Number of Layers\n",
    "\n",
    "We will use a total of 7 layers: 1 input, 5 hidden, and 1 output layer. Generally, as the depth of the network increases, the model performs better on the training data because it has a greater capacity. To prevent overfitting though, we do not want to make the model too deep. 5 hidden layers was selected based on observing the training curves and validation scores of recorded by different models across several building datasets. More layers generally tended to not improve performance on the validation set, and fewer layers meant the model was not able to learn even the training data. \n",
    "\n",
    "## Units per Layer\n",
    "\n",
    "The number of units per layers will be: 32, 64, 128, 256, 512, 1024, 1. The number of output units must be 1 because the network outputs a single prediction.\n",
    "\n",
    "## Activations\n",
    "\n",
    "The output layer in a regression problem must have no activation because we are interested in predicting a continous value that can take on any positive value (the output layer in a binary classification task has a `sigmoid` activation and the output layer for a multiclass classification task has a `softmax` activation).\n",
    "\n",
    "The current recommedation for developing fully-connected deep neural networks is to use a ReLU (rectified linear) activation on the hidden layers along with Batch Normalization. The \n",
    "definition of the ReLU function is $$f(x) = max(0, x)$$. This activation prevents the vanishing gradient problem common with saturating activation functions such as `tanh` or `sigmoid`. ReLU is very quick to train and has demonstrated good accuracy. The drawbacks are that it can lead to \"dead\" neurons because any value less than 0 is set to 0. Moreover, the mean activation will be greater than 0, which can lead to issues with training stability. The mean activation issue can be addressed by applying a Batch Normalization layer after each activation. There are a number of other activation functions which have been developed in recent years, but ReLU + Batch Norm remains a good default choice with fast training times and reasonable generalization to the test data. \n",
    "\n",
    "## Regularization Methods\n",
    "\n",
    "Regularization is used to address overfitting on the training data. This can be done by placing penalties on the L1 or L2 norm of the weight matrices that are added to the objective function. This encourages smaller magnitude weights which reduces the variance of the model. Other methods of regularization include data augmentation (typically for image data) or adding noise to the targets to encourage robustness to small perturbances. We will employ two methods of regularization: early stopping (discussion in the training iterations section below) and dropout. \n",
    "\n",
    "#### Dropout \n",
    "\n",
    "[Improving Neural Networks by Preventing Co-Adaptation of Feature Detectors](https://arxiv.org/pdf/1207.0580.pdf)\n",
    "\n",
    "Dropout has become one of the most popular methods for regularization. It is a techique that randomly drops (sets the activations to 0) a fraction of the units in a layer during training. The fraction is usually around 0.5 and can be different across layers. The idea behind dropout is it encourages the model to be robust and have less variance because neurons in a subsequent layer cannot depend on the neuron output in the previous  layer. Dropout is applied to the outputs of fully connected hidden layers typically before the activation function. Dropout must not be used during testing! (Keras will handle this for us automatically). We will use dropout with a rate of 0.5 after every hidden layer except for the final hidden layer before the output. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture Implementation\n",
    "\n",
    "Below is the code to create the basic architecture described above. We use fully connected layers with \"relu\" activations for all of the hidden layers followed by dropout with a rate of 0.5 (except for the final hidden layer) and batch norm applied after every hidden layer. The final output layer has no activation because this is a regression problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 32)                704       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 1025      \n",
      "=================================================================\n",
      "Total params: 710,145\n",
      "Trainable params: 706,113\n",
      "Non-trainable params: 4,032\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "\n",
    "# Input layer\n",
    "model.add(layers.Dense(32, activation=\"relu\", input_shape = (train.shape[1], )))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.BatchNormalization())\n",
    "\n",
    "# Five hidden layers\n",
    "model.add(layers.Dense(64, activation = \"relu\"))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dense(128, activation = \"relu\"))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dense(256, activation = \"relu\"))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dense(512, activation = \"relu\"))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dense(1024, activation = \"relu\"))\n",
    "model.add(layers.BatchNormalization())\n",
    "\n",
    "# Output layer\n",
    "model.add(layers.Dense(1, activation = None))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer\n",
    "\n",
    "All of the optimizers now used in neural networks are variants on vanilla gradient descent. For example, Nesterov momentum uses the concept of momentum based on exponentially decaying moving averages of past gradients to encourage stable training. Another family of optimizers, represented by AdaGrad, RMSProp, and Adam use adaptive learning rates to encourage faster training and better convergence. The currently recommended default optimizer is Adam (Adaptive Moment Estimation) which adaptively computes the learning rate for each individual parameter based of estimates of the first and second moments of the gradients. There are a number of hyperparamters associated with the Adam optimizer besides the learning rate, but these can usually be left at the defaults. \n",
    "\n",
    "### Learning Rate of the Optimizer\n",
    "\n",
    "A learning rate that is too slow will take too long to converge, while a learning rate that is too high will not lead to the model \"jumping\" around the optimum of the objective function. While a good rule is to leave the learning rate low and train for a large number of epochs, training can go faster by starting with a higher learning rate and gradually decreasing it over the number of epochs. This is called learning rate decay.  \n",
    "\n",
    "#### Learning Rate Decay\n",
    "\n",
    "Learning rate decay gradually decreases the learning rate as the model trains. The idea is that the optimizer can take large steps at the beginning of training and then smaller steps as it gets closer to an optimum of the objective function. Learning rate decay is often linear or exponential. However, with the Adam optimization algorithm, the __learning rate is adaptively computed for individual parameters__. When using Adam, a learning rate decay schedule is typically not employed because of the adaptive learning rate. \n",
    "\n",
    "We will therefore use the Adam optimizer with the default parameters in Keras:\n",
    "\n",
    "* `lr = 0.001`\n",
    "* `beta_1 = 0.9`\n",
    "* `beta_2 = 0.999`\n",
    "* `epsilon = None`\n",
    "* `decay = 0.0`\n",
    "* `amsgrad = False` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the optimizer\n",
    "opt = optimizers.Adam(lr = 0.001, beta_1 = 0.9, beta_2 = 0.999,\n",
    "                      epsilon = None, decay = 0.0, amsgrad = False)\n",
    "\n",
    "# Compile the model with specified optimizer\n",
    "model.compile(optimizer = opt, loss = \"mean_absolute_percentage_error\",\n",
    "              metrics = [\"mean_absolute_percentage_error\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of Training Epochs\n",
    "\n",
    "The number of training epochs is another crucial hyperparamter of a neural network. This represents the number of complete passes through the training data. Too few passes and the network will not learn the relationship, and too many passes and the network will overfit. Fortunately, there is a simple method to determine the best number of iterations that we have already employed when building a gradient boosting machine: early stopping.\n",
    "\n",
    "### Early Stopping\n",
    "\n",
    "Early stopping refers to training until the loss on a validation set does not decrease for a certain number of epochs. Early stopping is used to prevent overfitting by stopping training when the generalization error (as estimated by the validation set) has reached a minimum. We will implement early stopping by cutting off training after the validation loss has not decresed for 10 iterations. We will use 20% of the training data for validation (this will be randomly sampled from the data on every epoch). When implementing early stopping, we need to save the model parameters (weights and biases) every time the validation loss decreases. Then, when the training has stopped, we load the model weights that achieved the lowest error on the validation data. This model is then used to make precitions. Unfortunately this still requires writing the model weights to disk after every decrease in validation error which is not very efficient. Early stopping is always recommended as a simple method of regularization. The maximum number of epochs will be set at 100. \n",
    "\n",
    "## Batch Size \n",
    "\n",
    "The batch size refers to the number of training examples passed through the network at a time. All the neural network optimizers operate on minibatches of examples rather than processing the whole training set at once. Each batch is fed forward through the network, then backpropagation is used to calculate the gradients of the objective function with respect to the model parameters. Then the optimizer updates the weights according to the gradients and the learning rate. The next batch can then be passed through with the updated parameters. One pass of all the training data through the network is referred to as one epoch. Smaller batch sizes typically lead to better generalization performance although they may increase training time. Batch sizes are almost always a power of 2 to take advantage of computer architectures. We will use a batch size of 16 which means passing 16 training datapoints to the model at a time. The number of iterations per epoch will be (number of training datapoints - number of validation datapoints) / batch size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Implementation\n",
    "\n",
    "We will train the model implementing the above description. The model uses early stopping with a patience of 10 epochs and 20% of the data used for validation. We save a copy of the model weights to disk every time the validation loss decreases. Training information will be available in the `history` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 71416 samples, validate on 17855 samples\n",
      "Epoch 1/100\n",
      "71416/71416 [==============================] - 50s 700us/step - loss: 33.5104 - mean_absolute_percentage_error: 33.5104 - val_loss: 33.9968 - val_mean_absolute_percentage_error: 33.9968\n",
      "Epoch 2/100\n",
      "71416/71416 [==============================] - 49s 680us/step - loss: 22.1322 - mean_absolute_percentage_error: 22.1322 - val_loss: 29.0347 - val_mean_absolute_percentage_error: 29.0347\n",
      "Epoch 3/100\n",
      "71416/71416 [==============================] - 47s 660us/step - loss: 20.2166 - mean_absolute_percentage_error: 20.2166 - val_loss: 68.6281 - val_mean_absolute_percentage_error: 68.6281\n",
      "Epoch 4/100\n",
      "71416/71416 [==============================] - 49s 686us/step - loss: 19.0316 - mean_absolute_percentage_error: 19.0316 - val_loss: 41.4244 - val_mean_absolute_percentage_error: 41.4244\n",
      "Epoch 5/100\n",
      "71416/71416 [==============================] - 48s 666us/step - loss: 18.6532 - mean_absolute_percentage_error: 18.6532 - val_loss: 79.8662 - val_mean_absolute_percentage_error: 79.8662\n",
      "Epoch 6/100\n",
      "71416/71416 [==============================] - 48s 670us/step - loss: 17.7975 - mean_absolute_percentage_error: 17.7975 - val_loss: 66.6839 - val_mean_absolute_percentage_error: 66.6839\n",
      "Epoch 7/100\n",
      "71416/71416 [==============================] - 50s 695us/step - loss: 17.4435 - mean_absolute_percentage_error: 17.4435 - val_loss: 68.6557 - val_mean_absolute_percentage_error: 68.6557\n",
      "Epoch 8/100\n",
      "71416/71416 [==============================] - 55s 776us/step - loss: 17.1454 - mean_absolute_percentage_error: 17.1454 - val_loss: 33.1360 - val_mean_absolute_percentage_error: 33.1360\n",
      "Epoch 9/100\n",
      "71416/71416 [==============================] - 51s 716us/step - loss: 16.9902 - mean_absolute_percentage_error: 16.9902 - val_loss: 69.9849 - val_mean_absolute_percentage_error: 69.9849\n",
      "Epoch 10/100\n",
      "71416/71416 [==============================] - 48s 678us/step - loss: 16.7171 - mean_absolute_percentage_error: 16.7171 - val_loss: 45.7916 - val_mean_absolute_percentage_error: 45.7916\n",
      "Epoch 11/100\n",
      "71416/71416 [==============================] - 50s 695us/step - loss: 16.5480 - mean_absolute_percentage_error: 16.5480 - val_loss: 48.3490 - val_mean_absolute_percentage_error: 48.3490\n",
      "Epoch 12/100\n",
      "71416/71416 [==============================] - 50s 704us/step - loss: 16.8150 - mean_absolute_percentage_error: 16.8150 - val_loss: 67.3812 - val_mean_absolute_percentage_error: 67.3812\n"
     ]
    }
   ],
   "source": [
    "# Early stopping and model checkpoint\n",
    "callback_list = [callbacks.EarlyStopping(monitor = \"val_loss\", patience=10),\n",
    "                 callbacks.ModelCheckpoint(filepath = \"models/aps_model.h5\",\n",
    "                                           monitor = \"val_loss\",\n",
    "                                           save_best_only = True,\n",
    "                                           save_weights_only = True)]\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train, train_targets, batch_size = 32,\n",
    "                    callbacks=callback_list, epochs = 100,\n",
    "                    validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions \n",
    "\n",
    "To make predictions, we need to load in the best model weights (which we saved periodically during training). Then we simply called predict on the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model weights\n",
    "model.load_weights('models/aps_model.h5')\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = model.predict(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Training Curves\n",
    "\n",
    "One diagnostic tool of a deep neural network model are the training curves. These show the training and validation error as the epochs progress. Training curves allow us to determine if the model is overfitting or if the validation error was still decreasing when training stopped. Based on this information, if the model was overfitting, we could add regularization or decrease the capacity of the model (fewer layers / fewer neurons per layer). If the validation loss was still decreasing at the end of training, then we would want to increase the number of training epochs. The training and validation losses are saved in the `history` variable. We can write a short function to visualize the training curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training history of a keras model\n",
    "def plot_history(history):\n",
    "    val_loss = history.history['val_loss']\n",
    "    train_loss = history.history['loss']\n",
    "    epochs = [int(i) for i in list(range(1, len(val_loss) + 1))]\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    \n",
    "    plt.plot(epochs, train_loss, 'bo-', label = 'training loss')\n",
    "    plt.plot(epochs, val_loss, 'ro-', label = 'validation loss')\n",
    "    plt.xlabel('Epoch'); plt.ylabel('MAPE'); plt.title('Training Curves')\n",
    "    plt.legend();\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Network Function\n",
    "\n",
    "Now we can take the code and put it into a function. The function will take in the standard training features, training targets, testing features, testing targets and return the training time, prediction (inference) time, and the mape on the test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dnn_model(train, train_targets, test, test_targets, save_file):\n",
    "    \"\"\"Train a deep neural network and make predictions\n",
    "    \n",
    "    Parameters\n",
    "    --------\n",
    "    train : dataframe, shape = [n_training_samples, n_features]\n",
    "        Set of training features for training a model\n",
    "    \n",
    "    train_targets : array, shape = [n_training_samples]\n",
    "        Array of training targets for training a model\n",
    "        \n",
    "    test : dataframe, shape = [n_testing_samples, n_features]\n",
    "        Set of testing features for making predictions with a model\n",
    "    \n",
    "    test_targets : array, shape = [n_testing_samples]\n",
    "        Array of testing targets for evaluating the model predictions\n",
    "        \n",
    "    save_file : string\n",
    "        File name for saving the model weights. The model will be saved\n",
    "        to the directory models with the save file in the h5 format: models/save_file.h5\n",
    "        \n",
    "    Returns\n",
    "    --------\n",
    "    \n",
    "    results : array, shape = [4]\n",
    "        Numpy array of results. \n",
    "        First entry is the model, second is the training time,\n",
    "        third is the testing time, and fourth is the MAPE. All entries\n",
    "        are in strings and so will need to be converted to numbers.\n",
    "    \n",
    "    \"\"\"\n",
    "    model = models.Sequential()\n",
    "\n",
    "    # Input layer\n",
    "    model.add(layers.Dense(32, activation=\"relu\", input_shape = (train.shape[1], )))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.BatchNormalization())\n",
    "\n",
    "    # Five hidden layers\n",
    "    model.add(layers.Dense(64, activation = \"relu\"))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dense(128, activation = \"relu\"))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dense(256, activation = \"relu\"))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dense(512, activation = \"relu\"))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dense(1024, activation = \"relu\"))\n",
    "    model.add(layers.BatchNormalization())\n",
    "\n",
    "    # Output layer\n",
    "    model.add(layers.Dense(1, activation = None))\n",
    "    \n",
    "    # Define the optimizer\n",
    "    opt = optimizers.Adam(lr = 0.001, beta_1 = 0.9, beta_2 = 0.999,\n",
    "                          epsilon = None, decay = 0.0, amsgrad = False)\n",
    "\n",
    "    # Compile the model with specified optimizer\n",
    "    model.compile(optimizer = opt, loss = \"mean_absolute_percentage_error\",\n",
    "                  metrics = [\"mean_absolute_percentage_error\"])\n",
    "    \n",
    "    # Early stopping and model checkpoint\n",
    "    callback_list = [callbacks.EarlyStopping(monitor = \"val_loss\", patience=10),\n",
    "                     callbacks.ModelCheckpoint(filepath = \"models/%s.h5\" % save_file,\n",
    "                                               monitor = \"val_loss\",\n",
    "                                               save_best_only = True,\n",
    "                                               save_weights_only = True)]\n",
    "\n",
    "    # Start the training timer\n",
    "    start = timer()\n",
    "    # Train the model\n",
    "    history = model.fit(train, train_targets, batch_size = 32,\n",
    "                        callbacks=callback_list, epochs = 100,\n",
    "                        validation_split = 0.2)\n",
    "    \n",
    "    # Calculate the training time\n",
    "    end = timer()\n",
    "    train_time = end - start\n",
    "    \n",
    "    # Load the best validation model weights\n",
    "    model.load_weights('models/%s.h5' % save_file)\n",
    "\n",
    "    # Start the testing time\n",
    "    start = timer()\n",
    "    # Make predictions on the test data\n",
    "    predictions = model.predict(test)\n",
    "    \n",
    "    # Calculate the testing time\n",
    "    end = timer()\n",
    "    test_time = end - start\n",
    "    \n",
    "    # Calculate the mape\n",
    "    mape = 100 * np.mean( abs(predictions - test_targets) / test_targets)\n",
    "    \n",
    "    return np.array(['dnn', train_time, test_time, mape])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interface with Other Models\n",
    "\n",
    "The `dnn_model` function has the standard inputs and outputs for models we defined in the project. Therefore it can be easily integrated into the `evaluate_models` function with the six Scikit-Learn models and the Gradient Boosting Machine. This is the final model we will add to the retinue of models to evaluate on the EDIFES data. The next step will be running this function across hundreds of buildings and recording the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models(df):\n",
    "    \"\"\"Evaluate machine learning models\n",
    "    on a building energy dataset. More models can be added\n",
    "    to the function as required. \n",
    "    \n",
    "    \n",
    "    Parameters\n",
    "    --------\n",
    "    df : dataframe\n",
    "        Building energy dataframe. Each row must have one observation\n",
    "        and the columns must contain the features. The dataframe\n",
    "        needs to have an \"elec_cons\" column to be used as targets. \n",
    "    \n",
    "    Return\n",
    "    --------\n",
    "    results : dataframe, shape = [n_models, 4]\n",
    "        Modeling metrics. A dataframe with columns:\n",
    "        model, train_time, test_time, mape. Used for comparing\n",
    "        models for a given building dataset\n",
    "        \n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Preprocess the data for machine learning\n",
    "        train, train_targets, test, test_targets = preprocess_data(df, test_days = 183, scale = True)\n",
    "    except Exception as e:\n",
    "        print('Error processing data: ', e)\n",
    "        return\n",
    "        \n",
    "    # elasticnet\n",
    "    model = ElasticNet(alpha = 1.0, l1_ratio=0.5)\n",
    "    elasticnet_results = implement_model(model, train, train_targets, test, \n",
    "                                         test_targets, model_name = 'elasticnet')\n",
    "    \n",
    "    # knn\n",
    "    model = KNeighborsRegressor()\n",
    "    knn_results = implement_model(model, train, train_targets, test, \n",
    "                                  test_targets, model_name = 'knn')\n",
    "    \n",
    "    # svm\n",
    "    model = SVR()\n",
    "    svm_results = implement_model(model, train, train_targets, test, \n",
    "                                   test_targets, model_name = 'svm')\n",
    "    \n",
    "    # rf\n",
    "    model = RandomForestRegressor(n_estimators = 100, n_jobs = -1)\n",
    "    rf_results = implement_model(model, train, train_targets, test, \n",
    "                                  test_targets, model_name = 'rf')\n",
    "    \n",
    "    # et\n",
    "    model = ExtraTreesRegressor(n_estimators=100, n_jobs = -1)\n",
    "    et_results = implement_model(model, train, train_targets, test, \n",
    "                                  test_targets, model_name = 'et')\n",
    "    \n",
    "    # adaboost\n",
    "    model = AdaBoostRegressor(n_estimators = 1000, learning_rate = 0.05, \n",
    "                              loss = 'exponential')\n",
    "    adaboost_results = implement_model(model, train, train_targets, test, \n",
    "                                       test_targets, model_name = 'adaboost')\n",
    "    \n",
    "    # gbm\n",
    "    gbm_results = gbm_model(train, train_targets, test, test_targets)\n",
    "    \n",
    "    dnn_results = dnn_model(train, train_targets, test, test_targets, save_file = 'dnn_model')\n",
    "    \n",
    "    # Put the results into a single array (stack the rows)\n",
    "    results = np.vstack((elasticnet_results, knn_results, svm_results,\n",
    "                         rf_results, et_results, adaboost_results,\n",
    "                         gbm_results, dnn_results))\n",
    "    \n",
    "    # Convert the results to a dataframe\n",
    "    results = pd.DataFrame(results, columns = ['model', 'train_time', 'test_time', 'mape'])\n",
    "    \n",
    "    # Convert the numeric results to numbers\n",
    "    results.iloc[:, 1:] = results.iloc[:, 1:].astype(np.float32)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "In this notebook we built a deep neural network for use on the building prediction problem. We walked through the steps and the reasoning behind the design choices. The final model was then implemented in a function with the same set of standard inputs and outputs used throughout the project. We will now be able to use this function to evaluate all of the models on hundreds of buildings and choose the best model for further development. I will see you in the next notebook! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
